{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import os \n",
    "from torchvision.datasets import SVHN \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=SVHN(split='train',root='data/train',transform=ToTensor())\n",
    "test_dataset=SVHN(split='test',root='data/test',transform=ToTensor())\n",
    "\n",
    "batch_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional block: Conv + Norm + Act + Dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, add_norm=True, activation=\"ReLU\", dropout=None):\n",
    "        \"\"\" Module Initializer \"\"\"\n",
    "        super().__init__()\n",
    "        assert activation in [\"ReLU\", \"LeakyReLU\", \"Sigmoid\", \"Tanh\", None]\n",
    "        padding = kernel_size // 2\n",
    "        \n",
    "        block = []\n",
    "        block.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride))\n",
    "        if add_norm:\n",
    "            block.append(nn.BatchNorm2d(out_channels))\n",
    "        if activation is not None:\n",
    "            nonlinearity = getattr(nn, activation, nn.ReLU)()\n",
    "            if isinstance(nonlinearity, nn.LeakyReLU):\n",
    "                nonlinearity.negative_slope = 0.2\n",
    "            block.append(nonlinearity)\n",
    "            \n",
    "        if dropout is not None:\n",
    "            block.append(nn.Dropout(dropout))\n",
    "            \n",
    "        self.block =  nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        y = self.block(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ConvTransposeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional block: ConvTranspose + Norm + Act + Dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, add_norm=True, activation=\"ReLU\", dropout=None):\n",
    "        \"\"\" Module Initializer \"\"\"\n",
    "        super().__init__()\n",
    "        assert activation in [\"ReLU\", \"LeakyReLU\", \"Tanh\", None]\n",
    "        padding = kernel_size // 2\n",
    "        \n",
    "        block = []\n",
    "        block.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, stride=stride))\n",
    "        if add_norm:\n",
    "            block.append(nn.BatchNorm2d(out_channels))\n",
    "        if activation is not None:\n",
    "            nonlinearity = getattr(nn, activation, nn.ReLU)()\n",
    "            if isinstance(nonlinearity, nn.LeakyReLU):\n",
    "                nonlinearity.negative_slope = 0.2\n",
    "            block.append(nonlinearity)\n",
    "        if dropout is not None:\n",
    "            block.append(nn.Dropout(dropout))\n",
    "            \n",
    "        self.block =  nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        y = self.block(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    \"\"\" Reshaping a vector in a given shape \"\"\"\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        \"\"\" \"\"\"\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" \"\"\"\n",
    "        B, N = x.shape\n",
    "        x = x.view(B, N, 1, 1)\n",
    "        y = x.repeat(1, 1, *self.shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully convolutional generator using ReLU activations. \n",
    "    Takes as input a latent vector and outputs a fake sample.\n",
    "       (B, latent_dim, 1, 1)  --> (B, num_channels, 32, 32)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=128, num_channels=1, base_channels=32):\n",
    "        \"\"\" Model initializer \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(4):\n",
    "            layers.append(\n",
    "                ConvTransposeBlock(\n",
    "                        in_channels=latent_dim if i == 0 else base_channels * 2 ** (3-i+1),\n",
    "                        out_channels=base_channels * 2 ** (3-i),\n",
    "                        kernel_size=4,\n",
    "                        stride=1 if i == 0 else 2,\n",
    "                        add_norm=True,\n",
    "                        activation=\"ReLU\"\n",
    "                    )\n",
    "                )\n",
    "        layers.append(\n",
    "            ConvTransposeBlock(\n",
    "                    in_channels=base_channels,\n",
    "                    out_channels=num_channels,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    add_norm=False,\n",
    "                    activation=\"Tanh\"\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through generator \"\"\"\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim=128, num_channels=1, base_channels=32)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" A fully convolutional discriminator using LeakyReLU activations. \n",
    "    Takes as input either a real or fake sample and predicts its autenticity.\n",
    "       (B, num_channels, 32, 32)  -->  (B, 1, 1, 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_dim=1, base_channels=32, dropout=0.3):\n",
    "        \"\"\" Module initializer \"\"\"\n",
    "        super().__init__()  \n",
    "        \n",
    "        layers = []\n",
    "        for i in range(4):\n",
    "            layers.append(\n",
    "                ConvBlock(\n",
    "                        in_channels=in_channels if i == 0 else base_channels * 2 ** i,\n",
    "                        out_channels=base_channels * 2 ** (i + 1),\n",
    "                        kernel_size=4,\n",
    "                        add_norm=True,\n",
    "                        activation=\"LeakyReLU\",\n",
    "                        dropout=dropout,\n",
    "                        stride=2\n",
    "                    )\n",
    "                )\n",
    "        layers.append(\n",
    "                ConvBlock(\n",
    "                        in_channels=base_channels * 16,\n",
    "                        out_channels=out_dim,\n",
    "                        kernel_size=4,\n",
    "                        stride=4,\n",
    "                        add_norm=False,\n",
    "                        activation=\"Sigmoid\"\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        return\n",
    "      \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        y = self.model(x)\n",
    "        return y\n",
    "discriminator = Discriminator(in_channels=1, out_dim=1, base_channels=32)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim=128, num_channels=1, base_channels=32)\n",
    "discriminator = Discriminator(in_channels=1, out_dim=1, base_channels=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = torch.randn(16, 128, 1, 1)\n",
    "fake_img = generator(latent)\n",
    "score = discriminator(fake_img)\n",
    "print(f\"{fake_img.shape = }\")\n",
    "print(f\"{score.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def smooth(f, K=5):\n",
    "    \"\"\" Smoothing a function using a low-pass filter (mean) of size K \"\"\"\n",
    "    kernel = np.ones(K) / K\n",
    "    f = np.concatenate([f[:int(K//2)], f, f[int(-K//2):]])  # to account for boundaries\n",
    "    smooth_f = np.convolve(f, kernel, mode=\"same\")\n",
    "    smooth_f = smooth_f[K//2: -K//2]  # removing boundary-fixes\n",
    "    return smooth_f\n",
    "\n",
    "\n",
    "def count_model_params(model):\n",
    "    \"\"\" Counting the number of learnable parameters in a nn.Module \"\"\"\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return num_params\n",
    "\n",
    "def visualize_progress(loss_iters, train_loss, val_loss):\n",
    "    \"\"\" Visualizing loss and accuracy \"\"\"\n",
    "    fig, ax = plt.subplots(1,3)\n",
    "    fig.set_size_inches(24,5)\n",
    "\n",
    "    smooth_loss = smooth(loss_iters, 31)\n",
    "    ax[0].plot(loss_iters, c=\"blue\", label=\"Loss\", linewidth=3, alpha=0.5)\n",
    "    ax[0].plot(smooth_loss, c=\"red\", label=\"Smoothed Loss\", linewidth=3, alpha=1)\n",
    "    ax[0].legend(loc=\"best\")\n",
    "    ax[0].set_xlabel(\"Iteration\")\n",
    "    ax[0].set_ylabel(\"CE Loss\")\n",
    "    ax[0].set_yscale(\"log\")\n",
    "    ax[0].set_title(\"Training Progress\")\n",
    "\n",
    "    smooth_loss = smooth(loss_iters, 31)\n",
    "    START = 500\n",
    "    N_ITERS = len(loss_iters)\n",
    "    ax[1].plot(np.arange(START, N_ITERS), loss_iters[START:], c=\"blue\", label=\"Loss\", linewidth=3, alpha=0.5)\n",
    "    ax[1].plot(np.arange(START, N_ITERS), smooth_loss[START:], c=\"red\", label=\"Smoothed Loss\", linewidth=3, alpha=1)\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].set_ylabel(\"CE Loss\")\n",
    "    ax[1].set_yscale(\"log\")\n",
    "    ax[1].set_title(f\"Training Progress from Iter {START}\")\n",
    "\n",
    "    epochs = np.arange(len(train_loss)) + 1\n",
    "    ax[2].plot(epochs[1:], train_loss[1:], c=\"red\", label=\"Train Loss\", linewidth=3)\n",
    "    ax[2].plot(epochs[1:], val_loss[1:], c=\"blue\", label=\"Valid Loss\", linewidth=3)\n",
    "    ax[2].legend(loc=\"best\")\n",
    "    ax[2].set_xlabel(\"Epochs\")\n",
    "    ax[2].set_ylabel(\"CE Loss\")\n",
    "    ax[2].set_title(\"Loss Curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Class for initializing GAN and training it\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, discriminator, latent_dim=128):\n",
    "        \"\"\" Initialzer \"\"\"\n",
    "    \n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator = generator.to(self.device)\n",
    "        self.discriminator = discriminator.to(self.device)\n",
    "        \n",
    "        self.optim_discriminator = torch.optim.Adam(self.discriminator.parameters(), lr=3e-4, betas=(0.5, 0.9))\n",
    "        self.optim_generator = torch.optim.Adam(self.generator.parameters(), lr=3e-4, betas=(0.5, 0.9))\n",
    "        \n",
    "        # REAL LABEL = 1\n",
    "        # FAKE LABEL = 0\n",
    "        # eps = 1e-10\n",
    "        # self.criterion_d_real = lambda pred: torch.clip(-torch.log(1 - pred + eps), min=-10).mean()\n",
    "        # self.criterion_d_fake = lambda pred: torch.clip(-torch.log(pred + eps), min=-10).mean()\n",
    "        # self.criterion_g = lambda pred: torch.clip(-torch.log(1 - pred + eps), min=-10).mean()\n",
    "        \n",
    "        self.criterion_d_real = lambda pred: F.binary_cross_entropy(pred, torch.ones(pred.shape[0], device=pred.device))\n",
    "        self.criterion_d_fake = lambda pred: F.binary_cross_entropy(pred, torch.zeros(pred.shape[0], device=pred.device))\n",
    "        self.criterion_g = lambda pred: F.binary_cross_entropy(pred, torch.ones(pred.shape[0], device=pred.device))\n",
    "        \n",
    "        self.hist = {\n",
    "            \"d_real\": [],\n",
    "            \"d_fake\": [],\n",
    "            \"g\": []\n",
    "        }\n",
    "        return\n",
    "        \n",
    "    def train_one_step(self, imgs):\n",
    "        \"\"\" \n",
    "        raining both models for one optimization step\n",
    "        \"\"\"\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        # Sample from the latent distribution\n",
    "        B = imgs.shape[0]\n",
    "        latent = torch.randn(B, self.latent_dim, 1, 1).to(self.device)\n",
    "        \n",
    "        # ==== Training Discriminator ====\n",
    "        self.optim_discriminator.zero_grad()\n",
    "        # Get discriminator outputs for the real samples\n",
    "        prediction_real = self.discriminator(imgs)\n",
    "        # Compute the loss function\n",
    "        d_loss_real = self.criterion_d_real(prediction_real.view(B))\n",
    "\n",
    "        # Generating fake samples with the generator\n",
    "        fake_samples = self.generator(latent)\n",
    "        # Get discriminator outputs for the fake samples\n",
    "        prediction_fake_d = self.discriminator(fake_samples.detach())  # why detach?\n",
    "        # Compute the loss function\n",
    "        d_loss_fake = self.criterion_d_fake(prediction_fake_d.view(B))\n",
    "        (d_loss_real + d_loss_fake).backward()\n",
    "        assert fake_samples.shape == imgs.shape\n",
    "        \n",
    "        # optimization step\n",
    "        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 3.0)\n",
    "        self.optim_discriminator.step()\n",
    "        \n",
    "        # === Train the generator ===\n",
    "        self.optim_generator.zero_grad()\n",
    "        # Get discriminator outputs for the fake samples\n",
    "        prediction_fake_g = self.discriminator(fake_samples)\n",
    "        # Compute the loss function\n",
    "        g_loss = self.criterion_g(prediction_fake_g.view(B))\n",
    "        g_loss.backward()\n",
    "        # optimization step\n",
    "        self.optim_generator.step()\n",
    "        \n",
    "        return d_loss_real, d_loss_fake, g_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, N=64):\n",
    "        \"\"\" Generating a bunch of images using current state of generator \"\"\"\n",
    "        self.generator.eval()\n",
    "        latent = torch.randn(N, self.latent_dim, 1, 1).to(self.device)\n",
    "        imgs = self.generator(latent)\n",
    "        imgs = imgs * 0.5 + 0.5\n",
    "        return imgs\n",
    "        \n",
    "    def train(self, data_loader, N_iters=10000, init_step=0):\n",
    "        \"\"\" Training the models for several iterations \"\"\"\n",
    "        \n",
    "        progress_bar = tqdm(total=N_iters, initial=init_step)\n",
    "        running_d_loss = 0\n",
    "        running_g_loss = 0\n",
    "        \n",
    "        iter_ = 0\n",
    "        for i in range(N_iters):\n",
    "            for real_batch, _ in data_loader:           \n",
    "                real_batch = real_batch.to(self.device)\n",
    "                d_loss_real, d_loss_fake, g_loss = self.train_one_step(imgs=real_batch)\n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "            \n",
    "                # updating progress bar\n",
    "                progress_bar.set_description(f\"Ep {i+1} Iter {iter_}: D_Loss={round(d_loss.item(),5)}, G_Loss={round(g_loss.item(),5)})\")\n",
    "                \n",
    "\n",
    "    \n",
    "                if(iter_ % 200 == 0):\n",
    "                    imgs = self.generate()\n",
    "                    grid = torchvision.utils.make_grid(imgs, nrow=8)\n",
    "                    torchvision.utils.save_image(grid, os.path.join(os.getcwd(), \"imgs\", \"training\", f\"imgs_{iter_}.png\"))\n",
    "\n",
    "                iter_ = iter_ + 1 \n",
    "                \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim=128, num_channels=3, base_channels=32)\n",
    "discriminator = Discriminator(in_channels=3, out_dim=1, base_channels=32)\n",
    "\n",
    "trainer = Trainer(generator=generator, discriminator=discriminator, latent_dim=128)\n",
    "trainer.train(data_loader=train_loader, N_iters=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
