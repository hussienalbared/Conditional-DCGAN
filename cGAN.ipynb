{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/s-chh/Pytorch-cGAN-conditional-GAN/blob/main/cGAN.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import os\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import SVHN \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "GAN_LOGS = os.path.join(os.getcwd(), \"tboard_logs\", \"gan\")\n",
    "if  os.path.exists(GAN_LOGS):\n",
    "    shutil.rmtree(GAN_LOGS)\n",
    "if not os.path.exists(GAN_LOGS):\n",
    "    os.makedirs(GAN_LOGS)\n",
    "\n",
    "writer = SummaryWriter(GAN_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "BATCH_SIZE = 256\n",
    "Z_DIM = 10\n",
    "LABEL_EMBED_SIZE = 5\n",
    "NUM_CLASSES = 10\n",
    "IMGS_TO_DISPLAY_PER_CLASS = 20\n",
    "LOAD_MODEL = False\n",
    "\n",
    "CHANNELS = 3\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "\n",
    "dataset = SVHN(split='train',root='data/train',transform=transform,download=False)\n",
    "\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                        batch_size=BATCH_SIZE, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=4,\n",
    "                                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_path = os.path.join('samples')\n",
    "os.makedirs(samples_path, exist_ok=True)\n",
    "model_path=os.path.join('models')\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_imgs(generator,z, fixed_label, epoch=0):\n",
    "    generator.eval()\n",
    "    fake_imgs = generator(z, fixed_label)\n",
    "    fake_imgs = (fake_imgs + 1) / 2\n",
    "    fake_imgs_ = vutils.make_grid(fake_imgs, normalize=False, nrow=IMGS_TO_DISPLAY_PER_CLASS)\n",
    "    vutils.save_image(fake_imgs_, os.path.join(samples_path, 'sample_' + str(epoch) + '.png'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a conditional DCGAN model (https://arxiv.org/abs/1411.1784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for storing generated images\n",
    "\n",
    "\n",
    "\n",
    "# Networks\n",
    "def conv_block(c_in, c_out, k_size=4, stride=2, pad=1, use_bn=True, transpose=False):\n",
    "    module = []\n",
    "    if transpose:\n",
    "        module.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
    "    else:\n",
    "        module.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
    "    if use_bn:\n",
    "        module.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*module)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=10, num_classes=10, label_embed_size=5, channels=3, conv_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, label_embed_size)\n",
    "        self.tconv1 = conv_block(z_dim + label_embed_size, conv_dim * 4, pad=0, transpose=True)\n",
    "        self.tconv2 = conv_block(conv_dim * 4, conv_dim * 2, transpose=True)\n",
    "        self.tconv3 = conv_block(conv_dim * 2, conv_dim, transpose=True)\n",
    "        self.tconv4 = conv_block(conv_dim, channels, transpose=True, use_bn=False)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        x = x.reshape([x.shape[0], -1, 1, 1])\n",
    "        label_embed = self.label_embedding(label)\n",
    "        label_embed = label_embed.reshape([label_embed.shape[0], -1, 1, 1])\n",
    "        x = torch.cat((x, label_embed), dim=1)\n",
    "        x = F.relu(self.tconv1(x))\n",
    "        x = F.relu(self.tconv2(x))\n",
    "        x = F.relu(self.tconv3(x))\n",
    "        x = torch.tanh(self.tconv4(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=10, channels=3, conv_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_size = 32\n",
    "        self.label_embedding = nn.Embedding(num_classes, self.image_size*self.image_size)\n",
    "        self.conv1 = conv_block(channels + 1, conv_dim, use_bn=False)\n",
    "        self.conv2 = conv_block(conv_dim, conv_dim * 2)\n",
    "        self.conv3 = conv_block(conv_dim * 2, conv_dim * 4)\n",
    "        self.conv4 = conv_block(conv_dim * 4, 1, k_size=4, stride=1, pad=0, use_bn=False)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        alpha = 0.2\n",
    "        label_embed = self.label_embedding(label)\n",
    "        label_embed = label_embed.reshape([label_embed.shape[0], 1, self.image_size, self.image_size])\n",
    "        x = torch.cat((x, label_embed), dim=1)\n",
    "        x = F.leaky_relu(self.conv1(x), alpha)\n",
    "        x = F.leaky_relu(self.conv2(x), alpha)\n",
    "        x = F.leaky_relu(self.conv3(x), alpha)\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim=Z_DIM, num_classes=NUM_CLASSES, label_embed_size=LABEL_EMBED_SIZE, channels=CHANNELS)\n",
    "dis = Discriminator(num_classes=NUM_CLASSES, channels=CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous model   \n",
    "if LOAD_MODEL:\n",
    "    gen.load_state_dict(torch.load(os.path.join(model_path, 'gen.pkl')))\n",
    "    dis.load_state_dict(torch.load(os.path.join(model_path, 'dis.pkl')))\n",
    "\n",
    "\n",
    "\n",
    "# Define Optimizers\n",
    "g_opt = optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=2e-5)\n",
    "d_opt = optim.Adam(dis.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=2e-5)\n",
    "\n",
    "# Loss functions\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Fix images for viz\n",
    "fixed_z = torch.randn(IMGS_TO_DISPLAY_PER_CLASS*NUM_CLASSES, Z_DIM)\n",
    "fixed_label = torch.arange(0, NUM_CLASSES)\n",
    "fixed_label = torch.repeat_interleave(fixed_label, IMGS_TO_DISPLAY_PER_CLASS)\n",
    "\n",
    "# Labels\n",
    "real_label = torch.ones(BATCH_SIZE)\n",
    "fake_label = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "# GPU Compatibility\n",
    "\n",
    "\n",
    "gen, dis = gen.to(device), dis.to(device)\n",
    "real_label, fake_label = real_label.to(device), fake_label.to(device)\n",
    "fixed_z, fixed_label = fixed_z.to(device), fixed_label.to(device)\n",
    "\n",
    "\n",
    "max_iter = len(data_loader)\n",
    "\n",
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(EPOCHS,data_loader,generator,dis):\n",
    "    total_iters = 0\n",
    "    iter_ = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        generator.train()\n",
    "        dis.train()\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            total_iters += 1\n",
    "\n",
    "            # Loading data\n",
    "            x_real, x_label = data\n",
    "            z_fake = torch.randn(BATCH_SIZE, Z_DIM)\n",
    "\n",
    "            \n",
    "            x_real = x_real.to(device)\n",
    "            x_label = x_label.to(device)\n",
    "            z_fake = z_fake.to(device)\n",
    "\n",
    "            # Generate fake data\n",
    "            x_fake = generator(z_fake, x_label)\n",
    "\n",
    "            # Train Discriminator\n",
    "            fake_out = dis(x_fake.detach(), x_label)\n",
    "            real_out = dis(x_real.detach(), x_label)\n",
    "            d_loss_fake,d_loss_real=loss_fn(fake_out, fake_label) , loss_fn(real_out, real_label)\n",
    "            d_loss = (d_loss_fake+d_loss_real) / 2\n",
    "\n",
    "            d_opt.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_opt.step()\n",
    "\n",
    "            # Train Generator\n",
    "            fake_out = dis(x_fake, x_label)\n",
    "            g_loss = loss_fn(fake_out, real_label)\n",
    "\n",
    "            g_opt.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_opt.step()\n",
    "            writer.add_scalar(f'Loss/Generator Loss', g_loss.item(), global_step=iter_)\n",
    "            writer.add_scalar(f'Loss/Discriminator Loss', d_loss_real.item(), global_step=iter_)\n",
    "            writer.add_scalars(f'Loss/Discriminator Losses', {\n",
    "                        \"Real Images Loss\": d_loss_real.item(),\n",
    "                        \"Fake Images Loss\": d_loss_fake.item(),\n",
    "                    }, global_step=iter_)\n",
    "            writer.add_scalars(f'Comb_Loss/Losses', {\n",
    "                            'Discriminator': d_loss.item(),\n",
    "                            'Generator':  g_loss.item()\n",
    "                        }, iter_) \n",
    "            iter_=iter_+1\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(EPOCHS)\n",
    "                    + \"\\titer: \" + str(i) + \"/\" + str(max_iter)\n",
    "                    + \"\\ttotal_iters: \" + str(total_iters)\n",
    "                    + \"\\td_loss:\" + str(round(d_loss.item(), 4))\n",
    "                    + \"\\tg_loss:\" + str(round(g_loss.item(), 4))\n",
    "                    )\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            # torch.save(generator.state_dict(), os.path.join(model_path, 'gen.pkl'))\n",
    "            # torch.save(dis.state_dict(), os.path.join(model_path, 'dis.pkl'))\n",
    "\n",
    "            generate_imgs(generator,fixed_z, fixed_label, epoch)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model for conditional generation on the SVHN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\titer: 0/286\ttotal_iters: 1\td_loss:0.7383\tg_loss:1.543\n",
      "Epoch: 1/10\titer: 50/286\ttotal_iters: 51\td_loss:0.5566\tg_loss:1.6902\n",
      "Epoch: 1/10\titer: 100/286\ttotal_iters: 101\td_loss:0.3028\tg_loss:2.8035\n",
      "Epoch: 1/10\titer: 150/286\ttotal_iters: 151\td_loss:0.7431\tg_loss:1.5427\n",
      "Epoch: 1/10\titer: 200/286\ttotal_iters: 201\td_loss:0.3794\tg_loss:2.0135\n",
      "Epoch: 1/10\titer: 250/286\ttotal_iters: 251\td_loss:0.5455\tg_loss:3.4286\n",
      "Epoch: 2/10\titer: 0/286\ttotal_iters: 287\td_loss:0.4623\tg_loss:1.9361\n",
      "Epoch: 2/10\titer: 50/286\ttotal_iters: 337\td_loss:0.3853\tg_loss:2.0512\n",
      "Epoch: 2/10\titer: 100/286\ttotal_iters: 387\td_loss:0.5614\tg_loss:1.4795\n",
      "Epoch: 2/10\titer: 150/286\ttotal_iters: 437\td_loss:0.6659\tg_loss:1.435\n",
      "Epoch: 2/10\titer: 200/286\ttotal_iters: 487\td_loss:0.5784\tg_loss:1.6573\n",
      "Epoch: 2/10\titer: 250/286\ttotal_iters: 537\td_loss:0.4734\tg_loss:1.718\n",
      "Epoch: 3/10\titer: 0/286\ttotal_iters: 573\td_loss:0.4795\tg_loss:1.8408\n",
      "Epoch: 3/10\titer: 50/286\ttotal_iters: 623\td_loss:0.4141\tg_loss:2.298\n",
      "Epoch: 3/10\titer: 100/286\ttotal_iters: 673\td_loss:0.5549\tg_loss:1.9651\n",
      "Epoch: 3/10\titer: 150/286\ttotal_iters: 723\td_loss:0.6404\tg_loss:1.3269\n",
      "Epoch: 3/10\titer: 200/286\ttotal_iters: 773\td_loss:0.6667\tg_loss:1.2348\n",
      "Epoch: 3/10\titer: 250/286\ttotal_iters: 823\td_loss:0.5262\tg_loss:1.6908\n",
      "Epoch: 4/10\titer: 0/286\ttotal_iters: 859\td_loss:0.5479\tg_loss:2.2539\n",
      "Epoch: 4/10\titer: 50/286\ttotal_iters: 909\td_loss:0.4903\tg_loss:1.5289\n",
      "Epoch: 4/10\titer: 100/286\ttotal_iters: 959\td_loss:0.4558\tg_loss:2.1739\n",
      "Epoch: 4/10\titer: 150/286\ttotal_iters: 1009\td_loss:0.4574\tg_loss:1.322\n",
      "Epoch: 4/10\titer: 200/286\ttotal_iters: 1059\td_loss:0.3896\tg_loss:1.8466\n",
      "Epoch: 4/10\titer: 250/286\ttotal_iters: 1109\td_loss:0.5229\tg_loss:1.184\n",
      "Epoch: 5/10\titer: 0/286\ttotal_iters: 1145\td_loss:0.3592\tg_loss:2.0986\n",
      "Epoch: 5/10\titer: 50/286\ttotal_iters: 1195\td_loss:0.4879\tg_loss:3.2767\n",
      "Epoch: 5/10\titer: 100/286\ttotal_iters: 1245\td_loss:0.3539\tg_loss:2.4877\n",
      "Epoch: 5/10\titer: 150/286\ttotal_iters: 1295\td_loss:0.4162\tg_loss:1.7653\n",
      "Epoch: 5/10\titer: 200/286\ttotal_iters: 1345\td_loss:0.3976\tg_loss:1.3479\n",
      "Epoch: 5/10\titer: 250/286\ttotal_iters: 1395\td_loss:0.4704\tg_loss:2.9898\n",
      "Epoch: 6/10\titer: 0/286\ttotal_iters: 1431\td_loss:0.3849\tg_loss:2.3795\n",
      "Epoch: 6/10\titer: 50/286\ttotal_iters: 1481\td_loss:0.3783\tg_loss:1.6817\n",
      "Epoch: 6/10\titer: 100/286\ttotal_iters: 1531\td_loss:0.407\tg_loss:1.5953\n",
      "Epoch: 6/10\titer: 150/286\ttotal_iters: 1581\td_loss:0.3865\tg_loss:1.645\n",
      "Epoch: 6/10\titer: 200/286\ttotal_iters: 1631\td_loss:0.4164\tg_loss:1.679\n",
      "Epoch: 6/10\titer: 250/286\ttotal_iters: 1681\td_loss:0.5055\tg_loss:1.7132\n",
      "Epoch: 7/10\titer: 0/286\ttotal_iters: 1717\td_loss:0.4479\tg_loss:2.1041\n",
      "Epoch: 7/10\titer: 50/286\ttotal_iters: 1767\td_loss:0.5137\tg_loss:1.5896\n",
      "Epoch: 7/10\titer: 100/286\ttotal_iters: 1817\td_loss:0.664\tg_loss:0.9849\n",
      "Epoch: 7/10\titer: 150/286\ttotal_iters: 1867\td_loss:0.2659\tg_loss:2.3778\n",
      "Epoch: 7/10\titer: 200/286\ttotal_iters: 1917\td_loss:0.4293\tg_loss:2.3055\n",
      "Epoch: 7/10\titer: 250/286\ttotal_iters: 1967\td_loss:0.4093\tg_loss:2.3956\n",
      "Epoch: 8/10\titer: 0/286\ttotal_iters: 2003\td_loss:0.566\tg_loss:2.359\n",
      "Epoch: 8/10\titer: 50/286\ttotal_iters: 2053\td_loss:0.6012\tg_loss:1.0979\n",
      "Epoch: 8/10\titer: 100/286\ttotal_iters: 2103\td_loss:0.3953\tg_loss:1.6434\n",
      "Epoch: 8/10\titer: 150/286\ttotal_iters: 2153\td_loss:0.5313\tg_loss:1.9859\n",
      "Epoch: 8/10\titer: 200/286\ttotal_iters: 2203\td_loss:0.435\tg_loss:1.6295\n",
      "Epoch: 8/10\titer: 250/286\ttotal_iters: 2253\td_loss:0.5097\tg_loss:1.2723\n",
      "Epoch: 9/10\titer: 0/286\ttotal_iters: 2289\td_loss:0.4451\tg_loss:1.3231\n",
      "Epoch: 9/10\titer: 50/286\ttotal_iters: 2339\td_loss:0.3778\tg_loss:1.7554\n",
      "Epoch: 9/10\titer: 100/286\ttotal_iters: 2389\td_loss:0.3573\tg_loss:1.7134\n",
      "Epoch: 9/10\titer: 150/286\ttotal_iters: 2439\td_loss:0.5347\tg_loss:2.3642\n",
      "Epoch: 9/10\titer: 200/286\ttotal_iters: 2489\td_loss:0.3918\tg_loss:1.8307\n",
      "Epoch: 9/10\titer: 250/286\ttotal_iters: 2539\td_loss:0.9207\tg_loss:3.7104\n",
      "Epoch: 10/10\titer: 0/286\ttotal_iters: 2575\td_loss:0.3492\tg_loss:2.1851\n",
      "Epoch: 10/10\titer: 50/286\ttotal_iters: 2625\td_loss:0.3926\tg_loss:1.2675\n",
      "Epoch: 10/10\titer: 100/286\ttotal_iters: 2675\td_loss:0.3467\tg_loss:1.7043\n",
      "Epoch: 10/10\titer: 150/286\ttotal_iters: 2725\td_loss:0.4009\tg_loss:1.4586\n",
      "Epoch: 10/10\titer: 200/286\ttotal_iters: 2775\td_loss:0.357\tg_loss:1.9536\n",
      "Epoch: 10/10\titer: 250/286\ttotal_iters: 2825\td_loss:0.3559\tg_loss:1.7829\n"
     ]
    }
   ],
   "source": [
    "train(EPOCHS=EPOCHS,data_loader=data_loader,dis=dis,generator=gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the capabilities of the model to generate data based on given label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_imgs(gen,fixed_z, fixed_label, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kiwissen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
